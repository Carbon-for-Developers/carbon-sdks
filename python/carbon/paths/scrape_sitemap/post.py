# coding: utf-8

"""
    Carbon

    Connect external data to LLMs, no matter the source.

    The version of the OpenAPI document: 1.0.0
    Generated by: https://konfigthis.com
"""

from dataclasses import dataclass
import typing_extensions
import urllib3
from pydantic import RootModel
from carbon.request_before_hook import request_before_hook
import json
from urllib3._collections import HTTPHeaderDict

from carbon.api_response import AsyncGeneratorResponse
from carbon import api_client, exceptions
from datetime import date, datetime  # noqa: F401
import decimal  # noqa: F401
import functools  # noqa: F401
import io  # noqa: F401
import re  # noqa: F401
import typing  # noqa: F401
import typing_extensions  # noqa: F401
import uuid  # noqa: F401

import frozendict  # noqa: F401

from carbon import schemas  # noqa: F401

from carbon.model.http_validation_error import HTTPValidationError as HTTPValidationErrorSchema
from carbon.model.sitemap_scrape_request_css_classes_to_skip import SitemapScrapeRequestCssClassesToSkip as SitemapScrapeRequestCssClassesToSkipSchema
from carbon.model.sitemap_scrape_request_tags import SitemapScrapeRequestTags as SitemapScrapeRequestTagsSchema
from carbon.model.sitemap_scrape_request_css_selectors_to_skip import SitemapScrapeRequestCssSelectorsToSkip as SitemapScrapeRequestCssSelectorsToSkipSchema
from carbon.model.sitemap_scrape_request_urls_to_scrape import SitemapScrapeRequestUrlsToScrape as SitemapScrapeRequestUrlsToScrapeSchema
from carbon.model.sitemap_scrape_request_html_tags_to_skip import SitemapScrapeRequestHtmlTagsToSkip as SitemapScrapeRequestHtmlTagsToSkipSchema
from carbon.model.sitemap_scrape_request_url_paths_to_exclude import SitemapScrapeRequestUrlPathsToExclude as SitemapScrapeRequestUrlPathsToExcludeSchema
from carbon.model.embedding_generators import EmbeddingGenerators as EmbeddingGeneratorsSchema
from carbon.model.sitemap_scrape_request import SitemapScrapeRequest as SitemapScrapeRequestSchema
from carbon.model.sitemap_scrape_request_url_paths_to_include import SitemapScrapeRequestUrlPathsToInclude as SitemapScrapeRequestUrlPathsToIncludeSchema

from carbon.type.embedding_generators import EmbeddingGenerators
from carbon.type.sitemap_scrape_request_urls_to_scrape import SitemapScrapeRequestUrlsToScrape
from carbon.type.sitemap_scrape_request_tags import SitemapScrapeRequestTags
from carbon.type.http_validation_error import HTTPValidationError
from carbon.type.sitemap_scrape_request_url_paths_to_include import SitemapScrapeRequestUrlPathsToInclude
from carbon.type.sitemap_scrape_request_css_classes_to_skip import SitemapScrapeRequestCssClassesToSkip
from carbon.type.sitemap_scrape_request_url_paths_to_exclude import SitemapScrapeRequestUrlPathsToExclude
from carbon.type.sitemap_scrape_request import SitemapScrapeRequest
from carbon.type.sitemap_scrape_request_html_tags_to_skip import SitemapScrapeRequestHtmlTagsToSkip
from carbon.type.sitemap_scrape_request_css_selectors_to_skip import SitemapScrapeRequestCssSelectorsToSkip

from ...api_client import Dictionary
from carbon.pydantic.sitemap_scrape_request import SitemapScrapeRequest as SitemapScrapeRequestPydantic
from carbon.pydantic.sitemap_scrape_request_css_classes_to_skip import SitemapScrapeRequestCssClassesToSkip as SitemapScrapeRequestCssClassesToSkipPydantic
from carbon.pydantic.sitemap_scrape_request_url_paths_to_exclude import SitemapScrapeRequestUrlPathsToExclude as SitemapScrapeRequestUrlPathsToExcludePydantic
from carbon.pydantic.sitemap_scrape_request_url_paths_to_include import SitemapScrapeRequestUrlPathsToInclude as SitemapScrapeRequestUrlPathsToIncludePydantic
from carbon.pydantic.sitemap_scrape_request_urls_to_scrape import SitemapScrapeRequestUrlsToScrape as SitemapScrapeRequestUrlsToScrapePydantic
from carbon.pydantic.sitemap_scrape_request_html_tags_to_skip import SitemapScrapeRequestHtmlTagsToSkip as SitemapScrapeRequestHtmlTagsToSkipPydantic
from carbon.pydantic.http_validation_error import HTTPValidationError as HTTPValidationErrorPydantic
from carbon.pydantic.sitemap_scrape_request_tags import SitemapScrapeRequestTags as SitemapScrapeRequestTagsPydantic
from carbon.pydantic.embedding_generators import EmbeddingGenerators as EmbeddingGeneratorsPydantic
from carbon.pydantic.sitemap_scrape_request_css_selectors_to_skip import SitemapScrapeRequestCssSelectorsToSkip as SitemapScrapeRequestCssSelectorsToSkipPydantic

from . import path

# body param
SchemaForRequestBodyApplicationJson = SitemapScrapeRequestSchema


request_body_sitemap_scrape_request = api_client.RequestBody(
    content={
        'application/json': api_client.MediaType(
            schema=SchemaForRequestBodyApplicationJson),
    },
    required=True,
)
_auth = [
    'accessToken',
    'apiKey',
    'customerId',
]
SchemaFor200ResponseBodyApplicationJson = schemas.DictSchema


@dataclass
class ApiResponseFor200(api_client.ApiResponse):
    body: typing.Dict[str, typing.Union[bool, date, datetime, dict, float, int, list, str, None]]


@dataclass
class ApiResponseFor200Async(api_client.AsyncApiResponse):
    body: typing.Dict[str, typing.Union[bool, date, datetime, dict, float, int, list, str, None]]


_response_for_200 = api_client.OpenApiResponse(
    response_cls=ApiResponseFor200,
    response_cls_async=ApiResponseFor200Async,
    content={
        'application/json': api_client.MediaType(
            schema=SchemaFor200ResponseBodyApplicationJson),
    },
)
SchemaFor422ResponseBodyApplicationJson = HTTPValidationErrorSchema


@dataclass
class ApiResponseFor422(api_client.ApiResponse):
    body: HTTPValidationError


@dataclass
class ApiResponseFor422Async(api_client.AsyncApiResponse):
    body: HTTPValidationError


_response_for_422 = api_client.OpenApiResponse(
    response_cls=ApiResponseFor422,
    response_cls_async=ApiResponseFor422Async,
    content={
        'application/json': api_client.MediaType(
            schema=SchemaFor422ResponseBodyApplicationJson),
    },
)
_status_code_to_response = {
    '200': _response_for_200,
    '422': _response_for_422,
}
_all_accept_content_types = (
    'application/json',
)


class BaseApi(api_client.Api):

    def _scrape_sitemap_mapped_args(
        self,
        url: str,
        tags: typing.Optional[SitemapScrapeRequestTags] = None,
        max_pages_to_scrape: typing.Optional[typing.Optional[int]] = None,
        chunk_size: typing.Optional[typing.Optional[int]] = None,
        chunk_overlap: typing.Optional[typing.Optional[int]] = None,
        skip_embedding_generation: typing.Optional[typing.Optional[bool]] = None,
        enable_auto_sync: typing.Optional[typing.Optional[bool]] = None,
        generate_sparse_vectors: typing.Optional[typing.Optional[bool]] = None,
        prepend_filename_to_chunks: typing.Optional[typing.Optional[bool]] = None,
        html_tags_to_skip: typing.Optional[SitemapScrapeRequestHtmlTagsToSkip] = None,
        css_classes_to_skip: typing.Optional[SitemapScrapeRequestCssClassesToSkip] = None,
        css_selectors_to_skip: typing.Optional[SitemapScrapeRequestCssSelectorsToSkip] = None,
        embedding_model: typing.Optional[EmbeddingGenerators] = None,
        url_paths_to_include: typing.Optional[SitemapScrapeRequestUrlPathsToInclude] = None,
        url_paths_to_exclude: typing.Optional[SitemapScrapeRequestUrlPathsToExclude] = None,
        urls_to_scrape: typing.Optional[SitemapScrapeRequestUrlsToScrape] = None,
        download_css_and_media: typing.Optional[typing.Optional[bool]] = None,
        generate_chunks_only: typing.Optional[bool] = None,
        store_file_only: typing.Optional[bool] = None,
        use_premium_proxies: typing.Optional[bool] = None,
    ) -> api_client.MappedArgs:
        args: api_client.MappedArgs = api_client.MappedArgs()
        _body = {}
        if tags is not None:
            _body["tags"] = tags
        if url is not None:
            _body["url"] = url
        if max_pages_to_scrape is not None:
            _body["max_pages_to_scrape"] = max_pages_to_scrape
        if chunk_size is not None:
            _body["chunk_size"] = chunk_size
        if chunk_overlap is not None:
            _body["chunk_overlap"] = chunk_overlap
        if skip_embedding_generation is not None:
            _body["skip_embedding_generation"] = skip_embedding_generation
        if enable_auto_sync is not None:
            _body["enable_auto_sync"] = enable_auto_sync
        if generate_sparse_vectors is not None:
            _body["generate_sparse_vectors"] = generate_sparse_vectors
        if prepend_filename_to_chunks is not None:
            _body["prepend_filename_to_chunks"] = prepend_filename_to_chunks
        if html_tags_to_skip is not None:
            _body["html_tags_to_skip"] = html_tags_to_skip
        if css_classes_to_skip is not None:
            _body["css_classes_to_skip"] = css_classes_to_skip
        if css_selectors_to_skip is not None:
            _body["css_selectors_to_skip"] = css_selectors_to_skip
        if embedding_model is not None:
            _body["embedding_model"] = embedding_model
        if url_paths_to_include is not None:
            _body["url_paths_to_include"] = url_paths_to_include
        if url_paths_to_exclude is not None:
            _body["url_paths_to_exclude"] = url_paths_to_exclude
        if urls_to_scrape is not None:
            _body["urls_to_scrape"] = urls_to_scrape
        if download_css_and_media is not None:
            _body["download_css_and_media"] = download_css_and_media
        if generate_chunks_only is not None:
            _body["generate_chunks_only"] = generate_chunks_only
        if store_file_only is not None:
            _body["store_file_only"] = store_file_only
        if use_premium_proxies is not None:
            _body["use_premium_proxies"] = use_premium_proxies
        args.body = _body
        return args

    async def _ascrape_sitemap_oapg(
        self,
        body: typing.Any = None,
        skip_deserialization: bool = True,
        timeout: typing.Optional[typing.Union[float, typing.Tuple]] = None,
        accept_content_types: typing.Tuple[str] = _all_accept_content_types,
        content_type: str = 'application/json',
        stream: bool = False,
        **kwargs,
    ) -> typing.Union[
        ApiResponseFor200Async,
        api_client.ApiResponseWithoutDeserializationAsync,
        AsyncGeneratorResponse,
    ]:
        """
        Scrape Sitemap
        :param skip_deserialization: If true then api_response.response will be set but
            api_response.body and api_response.headers will not be deserialized into schema
            class instances
        """
        used_path = path.value
    
        _headers = HTTPHeaderDict()
        # TODO add cookie handling
        if accept_content_types:
            for accept_content_type in accept_content_types:
                _headers.add('Accept', accept_content_type)
        method = 'post'.upper()
        _headers.add('Content-Type', content_type)
    
        if body is schemas.unset:
            raise exceptions.ApiValueError(
                'The required body parameter has an invalid value of: unset. Set a valid value instead')
        _fields = None
        _body = None
        request_before_hook(
            resource_path=used_path,
            method=method,
            configuration=self.api_client.configuration,
            path_template='/scrape_sitemap',
            body=body,
            auth_settings=_auth,
            headers=_headers,
        )
        serialized_data = request_body_sitemap_scrape_request.serialize(body, content_type)
        if 'fields' in serialized_data:
            _fields = serialized_data['fields']
        elif 'body' in serialized_data:
            _body = serialized_data['body']
    
        response = await self.api_client.async_call_api(
            resource_path=used_path,
            method=method,
            headers=_headers,
            fields=_fields,
            serialized_body=_body,
            body=body,
            auth_settings=_auth,
            timeout=timeout,
            **kwargs
        )
    
        if stream:
            if not 200 <= response.http_response.status <= 299:
                body = (await response.http_response.content.read()).decode("utf-8")
                raise exceptions.ApiStreamingException(
                    status=response.http_response.status,
                    reason=response.http_response.reason,
                    body=body,
                )
    
            async def stream_iterator():
                """
                iterates over response.http_response.content and closes connection once iteration has finished
                """
                async for line in response.http_response.content:
                    if line == b'\r\n':
                        continue
                    yield line
                response.http_response.close()
                await response.session.close()
            return AsyncGeneratorResponse(
                content=stream_iterator(),
                headers=response.http_response.headers,
                status=response.http_response.status,
                response=response.http_response
            )
    
        response_for_status = _status_code_to_response.get(str(response.http_response.status))
        if response_for_status:
            api_response = await response_for_status.deserialize_async(
                                                    response,
                                                    self.api_client.configuration,
                                                    skip_deserialization=skip_deserialization
                                                )
        else:
            # If response data is JSON then deserialize for SDK consumer convenience
            is_json = api_client.JSONDetector._content_type_is_json(response.http_response.headers.get('Content-Type', ''))
            api_response = api_client.ApiResponseWithoutDeserializationAsync(
                body=await response.http_response.json() if is_json else await response.http_response.text(),
                response=response.http_response,
                round_trip_time=response.round_trip_time,
                status=response.http_response.status,
                headers=response.http_response.headers,
            )
    
        if not 200 <= api_response.status <= 299:
            raise exceptions.ApiException(api_response=api_response)
    
        # cleanup session / response
        response.http_response.close()
        await response.session.close()
    
        return api_response


    def _scrape_sitemap_oapg(
        self,
        body: typing.Any = None,
        skip_deserialization: bool = True,
        timeout: typing.Optional[typing.Union[float, typing.Tuple]] = None,
        accept_content_types: typing.Tuple[str] = _all_accept_content_types,
        content_type: str = 'application/json',
        stream: bool = False,
    ) -> typing.Union[
        ApiResponseFor200,
        api_client.ApiResponseWithoutDeserialization,
    ]:
        """
        Scrape Sitemap
        :param skip_deserialization: If true then api_response.response will be set but
            api_response.body and api_response.headers will not be deserialized into schema
            class instances
        """
        used_path = path.value
    
        _headers = HTTPHeaderDict()
        # TODO add cookie handling
        if accept_content_types:
            for accept_content_type in accept_content_types:
                _headers.add('Accept', accept_content_type)
        method = 'post'.upper()
        _headers.add('Content-Type', content_type)
    
        if body is schemas.unset:
            raise exceptions.ApiValueError(
                'The required body parameter has an invalid value of: unset. Set a valid value instead')
        _fields = None
        _body = None
        request_before_hook(
            resource_path=used_path,
            method=method,
            configuration=self.api_client.configuration,
            path_template='/scrape_sitemap',
            body=body,
            auth_settings=_auth,
            headers=_headers,
        )
        serialized_data = request_body_sitemap_scrape_request.serialize(body, content_type)
        if 'fields' in serialized_data:
            _fields = serialized_data['fields']
        elif 'body' in serialized_data:
            _body = serialized_data['body']
    
        response = self.api_client.call_api(
            resource_path=used_path,
            method=method,
            headers=_headers,
            fields=_fields,
            serialized_body=_body,
            body=body,
            auth_settings=_auth,
            timeout=timeout,
        )
    
        response_for_status = _status_code_to_response.get(str(response.http_response.status))
        if response_for_status:
            api_response = response_for_status.deserialize(
                                                    response,
                                                    self.api_client.configuration,
                                                    skip_deserialization=skip_deserialization
                                                )
        else:
            # If response data is JSON then deserialize for SDK consumer convenience
            is_json = api_client.JSONDetector._content_type_is_json(response.http_response.headers.get('Content-Type', ''))
            api_response = api_client.ApiResponseWithoutDeserialization(
                body=json.loads(response.http_response.data) if is_json else response.http_response.data,
                response=response.http_response,
                round_trip_time=response.round_trip_time,
                status=response.http_response.status,
                headers=response.http_response.headers,
            )
    
        if not 200 <= api_response.status <= 299:
            raise exceptions.ApiException(api_response=api_response)
    
        return api_response


class ScrapeSitemapRaw(BaseApi):
    # this class is used by api classes that refer to endpoints with operationId fn names

    async def ascrape_sitemap(
        self,
        url: str,
        tags: typing.Optional[SitemapScrapeRequestTags] = None,
        max_pages_to_scrape: typing.Optional[typing.Optional[int]] = None,
        chunk_size: typing.Optional[typing.Optional[int]] = None,
        chunk_overlap: typing.Optional[typing.Optional[int]] = None,
        skip_embedding_generation: typing.Optional[typing.Optional[bool]] = None,
        enable_auto_sync: typing.Optional[typing.Optional[bool]] = None,
        generate_sparse_vectors: typing.Optional[typing.Optional[bool]] = None,
        prepend_filename_to_chunks: typing.Optional[typing.Optional[bool]] = None,
        html_tags_to_skip: typing.Optional[SitemapScrapeRequestHtmlTagsToSkip] = None,
        css_classes_to_skip: typing.Optional[SitemapScrapeRequestCssClassesToSkip] = None,
        css_selectors_to_skip: typing.Optional[SitemapScrapeRequestCssSelectorsToSkip] = None,
        embedding_model: typing.Optional[EmbeddingGenerators] = None,
        url_paths_to_include: typing.Optional[SitemapScrapeRequestUrlPathsToInclude] = None,
        url_paths_to_exclude: typing.Optional[SitemapScrapeRequestUrlPathsToExclude] = None,
        urls_to_scrape: typing.Optional[SitemapScrapeRequestUrlsToScrape] = None,
        download_css_and_media: typing.Optional[typing.Optional[bool]] = None,
        generate_chunks_only: typing.Optional[bool] = None,
        store_file_only: typing.Optional[bool] = None,
        use_premium_proxies: typing.Optional[bool] = None,
        **kwargs,
    ) -> typing.Union[
        ApiResponseFor200Async,
        api_client.ApiResponseWithoutDeserializationAsync,
        AsyncGeneratorResponse,
    ]:
        args = self._scrape_sitemap_mapped_args(
            url=url,
            tags=tags,
            max_pages_to_scrape=max_pages_to_scrape,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            skip_embedding_generation=skip_embedding_generation,
            enable_auto_sync=enable_auto_sync,
            generate_sparse_vectors=generate_sparse_vectors,
            prepend_filename_to_chunks=prepend_filename_to_chunks,
            html_tags_to_skip=html_tags_to_skip,
            css_classes_to_skip=css_classes_to_skip,
            css_selectors_to_skip=css_selectors_to_skip,
            embedding_model=embedding_model,
            url_paths_to_include=url_paths_to_include,
            url_paths_to_exclude=url_paths_to_exclude,
            urls_to_scrape=urls_to_scrape,
            download_css_and_media=download_css_and_media,
            generate_chunks_only=generate_chunks_only,
            store_file_only=store_file_only,
            use_premium_proxies=use_premium_proxies,
        )
        return await self._ascrape_sitemap_oapg(
            body=args.body,
            **kwargs,
        )
    
    def scrape_sitemap(
        self,
        url: str,
        tags: typing.Optional[SitemapScrapeRequestTags] = None,
        max_pages_to_scrape: typing.Optional[typing.Optional[int]] = None,
        chunk_size: typing.Optional[typing.Optional[int]] = None,
        chunk_overlap: typing.Optional[typing.Optional[int]] = None,
        skip_embedding_generation: typing.Optional[typing.Optional[bool]] = None,
        enable_auto_sync: typing.Optional[typing.Optional[bool]] = None,
        generate_sparse_vectors: typing.Optional[typing.Optional[bool]] = None,
        prepend_filename_to_chunks: typing.Optional[typing.Optional[bool]] = None,
        html_tags_to_skip: typing.Optional[SitemapScrapeRequestHtmlTagsToSkip] = None,
        css_classes_to_skip: typing.Optional[SitemapScrapeRequestCssClassesToSkip] = None,
        css_selectors_to_skip: typing.Optional[SitemapScrapeRequestCssSelectorsToSkip] = None,
        embedding_model: typing.Optional[EmbeddingGenerators] = None,
        url_paths_to_include: typing.Optional[SitemapScrapeRequestUrlPathsToInclude] = None,
        url_paths_to_exclude: typing.Optional[SitemapScrapeRequestUrlPathsToExclude] = None,
        urls_to_scrape: typing.Optional[SitemapScrapeRequestUrlsToScrape] = None,
        download_css_and_media: typing.Optional[typing.Optional[bool]] = None,
        generate_chunks_only: typing.Optional[bool] = None,
        store_file_only: typing.Optional[bool] = None,
        use_premium_proxies: typing.Optional[bool] = None,
    ) -> typing.Union[
        ApiResponseFor200,
        api_client.ApiResponseWithoutDeserialization,
    ]:
        """ Extracts all URLs from a sitemap and performs a web scrape on each of them.  Args:     sitemap_url (str): URL of the sitemap  Returns:     dict: A response object with the status of the scraping job message.--> """
        args = self._scrape_sitemap_mapped_args(
            url=url,
            tags=tags,
            max_pages_to_scrape=max_pages_to_scrape,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            skip_embedding_generation=skip_embedding_generation,
            enable_auto_sync=enable_auto_sync,
            generate_sparse_vectors=generate_sparse_vectors,
            prepend_filename_to_chunks=prepend_filename_to_chunks,
            html_tags_to_skip=html_tags_to_skip,
            css_classes_to_skip=css_classes_to_skip,
            css_selectors_to_skip=css_selectors_to_skip,
            embedding_model=embedding_model,
            url_paths_to_include=url_paths_to_include,
            url_paths_to_exclude=url_paths_to_exclude,
            urls_to_scrape=urls_to_scrape,
            download_css_and_media=download_css_and_media,
            generate_chunks_only=generate_chunks_only,
            store_file_only=store_file_only,
            use_premium_proxies=use_premium_proxies,
        )
        return self._scrape_sitemap_oapg(
            body=args.body,
        )

class ScrapeSitemap(BaseApi):

    async def ascrape_sitemap(
        self,
        url: str,
        tags: typing.Optional[SitemapScrapeRequestTags] = None,
        max_pages_to_scrape: typing.Optional[typing.Optional[int]] = None,
        chunk_size: typing.Optional[typing.Optional[int]] = None,
        chunk_overlap: typing.Optional[typing.Optional[int]] = None,
        skip_embedding_generation: typing.Optional[typing.Optional[bool]] = None,
        enable_auto_sync: typing.Optional[typing.Optional[bool]] = None,
        generate_sparse_vectors: typing.Optional[typing.Optional[bool]] = None,
        prepend_filename_to_chunks: typing.Optional[typing.Optional[bool]] = None,
        html_tags_to_skip: typing.Optional[SitemapScrapeRequestHtmlTagsToSkip] = None,
        css_classes_to_skip: typing.Optional[SitemapScrapeRequestCssClassesToSkip] = None,
        css_selectors_to_skip: typing.Optional[SitemapScrapeRequestCssSelectorsToSkip] = None,
        embedding_model: typing.Optional[EmbeddingGenerators] = None,
        url_paths_to_include: typing.Optional[SitemapScrapeRequestUrlPathsToInclude] = None,
        url_paths_to_exclude: typing.Optional[SitemapScrapeRequestUrlPathsToExclude] = None,
        urls_to_scrape: typing.Optional[SitemapScrapeRequestUrlsToScrape] = None,
        download_css_and_media: typing.Optional[typing.Optional[bool]] = None,
        generate_chunks_only: typing.Optional[bool] = None,
        store_file_only: typing.Optional[bool] = None,
        use_premium_proxies: typing.Optional[bool] = None,
        validate: bool = False,
        **kwargs,
    ) -> Dictionary:
        raw_response = await self.raw.ascrape_sitemap(
            url=url,
            tags=tags,
            max_pages_to_scrape=max_pages_to_scrape,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            skip_embedding_generation=skip_embedding_generation,
            enable_auto_sync=enable_auto_sync,
            generate_sparse_vectors=generate_sparse_vectors,
            prepend_filename_to_chunks=prepend_filename_to_chunks,
            html_tags_to_skip=html_tags_to_skip,
            css_classes_to_skip=css_classes_to_skip,
            css_selectors_to_skip=css_selectors_to_skip,
            embedding_model=embedding_model,
            url_paths_to_include=url_paths_to_include,
            url_paths_to_exclude=url_paths_to_exclude,
            urls_to_scrape=urls_to_scrape,
            download_css_and_media=download_css_and_media,
            generate_chunks_only=generate_chunks_only,
            store_file_only=store_file_only,
            use_premium_proxies=use_premium_proxies,
            **kwargs,
        )
        if validate:
            return Dictionary(**raw_response.body)
        return api_client.construct_model_instance(Dictionary, raw_response.body)
    
    
    def scrape_sitemap(
        self,
        url: str,
        tags: typing.Optional[SitemapScrapeRequestTags] = None,
        max_pages_to_scrape: typing.Optional[typing.Optional[int]] = None,
        chunk_size: typing.Optional[typing.Optional[int]] = None,
        chunk_overlap: typing.Optional[typing.Optional[int]] = None,
        skip_embedding_generation: typing.Optional[typing.Optional[bool]] = None,
        enable_auto_sync: typing.Optional[typing.Optional[bool]] = None,
        generate_sparse_vectors: typing.Optional[typing.Optional[bool]] = None,
        prepend_filename_to_chunks: typing.Optional[typing.Optional[bool]] = None,
        html_tags_to_skip: typing.Optional[SitemapScrapeRequestHtmlTagsToSkip] = None,
        css_classes_to_skip: typing.Optional[SitemapScrapeRequestCssClassesToSkip] = None,
        css_selectors_to_skip: typing.Optional[SitemapScrapeRequestCssSelectorsToSkip] = None,
        embedding_model: typing.Optional[EmbeddingGenerators] = None,
        url_paths_to_include: typing.Optional[SitemapScrapeRequestUrlPathsToInclude] = None,
        url_paths_to_exclude: typing.Optional[SitemapScrapeRequestUrlPathsToExclude] = None,
        urls_to_scrape: typing.Optional[SitemapScrapeRequestUrlsToScrape] = None,
        download_css_and_media: typing.Optional[typing.Optional[bool]] = None,
        generate_chunks_only: typing.Optional[bool] = None,
        store_file_only: typing.Optional[bool] = None,
        use_premium_proxies: typing.Optional[bool] = None,
        validate: bool = False,
    ) -> Dictionary:
        raw_response = self.raw.scrape_sitemap(
            url=url,
            tags=tags,
            max_pages_to_scrape=max_pages_to_scrape,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            skip_embedding_generation=skip_embedding_generation,
            enable_auto_sync=enable_auto_sync,
            generate_sparse_vectors=generate_sparse_vectors,
            prepend_filename_to_chunks=prepend_filename_to_chunks,
            html_tags_to_skip=html_tags_to_skip,
            css_classes_to_skip=css_classes_to_skip,
            css_selectors_to_skip=css_selectors_to_skip,
            embedding_model=embedding_model,
            url_paths_to_include=url_paths_to_include,
            url_paths_to_exclude=url_paths_to_exclude,
            urls_to_scrape=urls_to_scrape,
            download_css_and_media=download_css_and_media,
            generate_chunks_only=generate_chunks_only,
            store_file_only=store_file_only,
            use_premium_proxies=use_premium_proxies,
        )
        if validate:
            return Dictionary(**raw_response.body)
        return api_client.construct_model_instance(Dictionary, raw_response.body)


class ApiForpost(BaseApi):
    # this class is used by api classes that refer to endpoints by path and http method names

    async def apost(
        self,
        url: str,
        tags: typing.Optional[SitemapScrapeRequestTags] = None,
        max_pages_to_scrape: typing.Optional[typing.Optional[int]] = None,
        chunk_size: typing.Optional[typing.Optional[int]] = None,
        chunk_overlap: typing.Optional[typing.Optional[int]] = None,
        skip_embedding_generation: typing.Optional[typing.Optional[bool]] = None,
        enable_auto_sync: typing.Optional[typing.Optional[bool]] = None,
        generate_sparse_vectors: typing.Optional[typing.Optional[bool]] = None,
        prepend_filename_to_chunks: typing.Optional[typing.Optional[bool]] = None,
        html_tags_to_skip: typing.Optional[SitemapScrapeRequestHtmlTagsToSkip] = None,
        css_classes_to_skip: typing.Optional[SitemapScrapeRequestCssClassesToSkip] = None,
        css_selectors_to_skip: typing.Optional[SitemapScrapeRequestCssSelectorsToSkip] = None,
        embedding_model: typing.Optional[EmbeddingGenerators] = None,
        url_paths_to_include: typing.Optional[SitemapScrapeRequestUrlPathsToInclude] = None,
        url_paths_to_exclude: typing.Optional[SitemapScrapeRequestUrlPathsToExclude] = None,
        urls_to_scrape: typing.Optional[SitemapScrapeRequestUrlsToScrape] = None,
        download_css_and_media: typing.Optional[typing.Optional[bool]] = None,
        generate_chunks_only: typing.Optional[bool] = None,
        store_file_only: typing.Optional[bool] = None,
        use_premium_proxies: typing.Optional[bool] = None,
        **kwargs,
    ) -> typing.Union[
        ApiResponseFor200Async,
        api_client.ApiResponseWithoutDeserializationAsync,
        AsyncGeneratorResponse,
    ]:
        args = self._scrape_sitemap_mapped_args(
            url=url,
            tags=tags,
            max_pages_to_scrape=max_pages_to_scrape,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            skip_embedding_generation=skip_embedding_generation,
            enable_auto_sync=enable_auto_sync,
            generate_sparse_vectors=generate_sparse_vectors,
            prepend_filename_to_chunks=prepend_filename_to_chunks,
            html_tags_to_skip=html_tags_to_skip,
            css_classes_to_skip=css_classes_to_skip,
            css_selectors_to_skip=css_selectors_to_skip,
            embedding_model=embedding_model,
            url_paths_to_include=url_paths_to_include,
            url_paths_to_exclude=url_paths_to_exclude,
            urls_to_scrape=urls_to_scrape,
            download_css_and_media=download_css_and_media,
            generate_chunks_only=generate_chunks_only,
            store_file_only=store_file_only,
            use_premium_proxies=use_premium_proxies,
        )
        return await self._ascrape_sitemap_oapg(
            body=args.body,
            **kwargs,
        )
    
    def post(
        self,
        url: str,
        tags: typing.Optional[SitemapScrapeRequestTags] = None,
        max_pages_to_scrape: typing.Optional[typing.Optional[int]] = None,
        chunk_size: typing.Optional[typing.Optional[int]] = None,
        chunk_overlap: typing.Optional[typing.Optional[int]] = None,
        skip_embedding_generation: typing.Optional[typing.Optional[bool]] = None,
        enable_auto_sync: typing.Optional[typing.Optional[bool]] = None,
        generate_sparse_vectors: typing.Optional[typing.Optional[bool]] = None,
        prepend_filename_to_chunks: typing.Optional[typing.Optional[bool]] = None,
        html_tags_to_skip: typing.Optional[SitemapScrapeRequestHtmlTagsToSkip] = None,
        css_classes_to_skip: typing.Optional[SitemapScrapeRequestCssClassesToSkip] = None,
        css_selectors_to_skip: typing.Optional[SitemapScrapeRequestCssSelectorsToSkip] = None,
        embedding_model: typing.Optional[EmbeddingGenerators] = None,
        url_paths_to_include: typing.Optional[SitemapScrapeRequestUrlPathsToInclude] = None,
        url_paths_to_exclude: typing.Optional[SitemapScrapeRequestUrlPathsToExclude] = None,
        urls_to_scrape: typing.Optional[SitemapScrapeRequestUrlsToScrape] = None,
        download_css_and_media: typing.Optional[typing.Optional[bool]] = None,
        generate_chunks_only: typing.Optional[bool] = None,
        store_file_only: typing.Optional[bool] = None,
        use_premium_proxies: typing.Optional[bool] = None,
    ) -> typing.Union[
        ApiResponseFor200,
        api_client.ApiResponseWithoutDeserialization,
    ]:
        """ Extracts all URLs from a sitemap and performs a web scrape on each of them.  Args:     sitemap_url (str): URL of the sitemap  Returns:     dict: A response object with the status of the scraping job message.--> """
        args = self._scrape_sitemap_mapped_args(
            url=url,
            tags=tags,
            max_pages_to_scrape=max_pages_to_scrape,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            skip_embedding_generation=skip_embedding_generation,
            enable_auto_sync=enable_auto_sync,
            generate_sparse_vectors=generate_sparse_vectors,
            prepend_filename_to_chunks=prepend_filename_to_chunks,
            html_tags_to_skip=html_tags_to_skip,
            css_classes_to_skip=css_classes_to_skip,
            css_selectors_to_skip=css_selectors_to_skip,
            embedding_model=embedding_model,
            url_paths_to_include=url_paths_to_include,
            url_paths_to_exclude=url_paths_to_exclude,
            urls_to_scrape=urls_to_scrape,
            download_css_and_media=download_css_and_media,
            generate_chunks_only=generate_chunks_only,
            store_file_only=store_file_only,
            use_premium_proxies=use_premium_proxies,
        )
        return self._scrape_sitemap_oapg(
            body=args.body,
        )

